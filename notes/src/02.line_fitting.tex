\section{Line Fitting}
\subsection{Least Square Method}
\begin{itemize}
    \item Data points: $(x_i, y_i)$, $i=1,2,\ldots,n$
    \item Line equation: $y_i - mx_i - b = 0$
    \item Objective: minimize the sum of squared errors ($\mathcal{L}_2$ norm): 
    \[
    \text{Energy} = \sum_{i=1}^{n} (y_i - mx_i - b)^2
    \]
\end{itemize}
对于不同的范数选择, energy landscape 会有所不同, 使用 $\mathcal{L}_2$ norm 我们可以得到一个解析解:
\begin{align*}
    E = \norm{ \mathbf{Y} - \mathbf{X} \mathbf{B} }^2 &= T^T Y - 2 (XB)^T Y + (XB)^T (XB) \\
    \frac{\partial E}{\partial B} &= - 2 X^T Y + 2 X^T X B = 0  \\
    \implies B^* &= (X^T X)^{-1} X^T Y \quad \text{where } B := [m, b]^T
\end{align*}
注: 关于矩阵求导可以参考 \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{Matrix Cookbook} 或 \href{https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf}{Matrix Calculus}.
\subsubsection{SVD Decomposition}
原始方法对于垂直线失效。$\implies$ 因此改用 $ax + by + d = 0$，此时优化问题变为：\begin{align*}
    \text{minimize} &\quad \norm{A h} = 0 \\ 
    \text{subject to } &\quad \norm{h} = 1
\end{align*}
其中 
\[
    \renewcommand{\arraystretch}{0.85}
    A = \begin{bmatrix}
        x_1 & y_1 & 1 \\
        x_2 & y_2 & 1\\
        & \cdots &  \\
        x_n & y_n & 1
    \end{bmatrix}, \quad h = \begin{bmatrix}
        a \\ b \\ d
    \end{bmatrix}
\]
我们可以使用 SVD 分解来求解 $h$. \marginpar{\kaishu 关于奇异值分解的详细内容可以参考附录.}
\[
    A_{n\times 3} = U_{n\times n} \Sigma_{n\times 3} V_{3\times 3}^T 
\]
其中 $U_{n\times n}$ 和 $V_{3\times 3}$ 是正交矩阵, 不妨设
\[
    \Sigma = \begin{bmatrix}
        \diag\{\lambda_1, \lambda_2, \lambda_3\} \\
        0
    \end{bmatrix}, \quad |\lambda_1| > |\lambda_2| > |\lambda_3|
\]
设 $V = [c_1, c_2, c_3]$, 其中 $\{c_i\}$ 构成了 $R^3$ 的正交基, 那么存在 $\alpha_1, \alpha_2, \alpha_3$ 使得
$ h = \alpha_1 c_1 + \alpha_2 c_2 + \alpha_3 c_3$ 且 $\alpha_1^2 + \alpha_2^2 + \alpha_3^2 = 1$.
进而:
\begin{align*}
    Ah = U \Sigma V^T h = U \Sigma \begin{bmatrix}
        \alpha_1 \\ \alpha_2 \\ \alpha_3
    \end{bmatrix} = \begin{bmatrix}
        \diag\{\lambda_1 \alpha_1, \lambda_2 \alpha_2, \lambda_3 \alpha_3\} \\
        0
    \end{bmatrix} \implies \norm{Ah}^2 = \sum_{i=1}^{3} \lambda_i^2 \alpha_i^2 \ge \lambda_3^2
\end{align*} 
注意到在 $h = c_3$ 时取等号, 因此最优解为 $h = c_3$.


\subsubsection{Robustness}
Least square method is robust to small noises but sensitive to outliers.

$\mathcal{L}_1$ norm 对于outlier的鲁棒性会更强(梯度与残差的大小不相关), 但优化问题更困难.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/not_roboust_outliner.png}
    \caption{Least Square Method is not robust to outliers}
\end{figure}


\subsection{RANSAC}

RANSAC:RANdom SAmple Consensus
\subsubsection{Ideas and Algorithm}
\textbf{Idea:} we need to ﬁnd a line that has the largest supporters (or inliers)
\marginpar{\kaishu 在作业中会实现并行算法.}
\begin{algo}
    \centering 
    \caption{\textbf{RANSAC Loop(Sequential Version)}}
    \begin{algorithmic}[1]
        \Require 假设这个直线 (平面) 需要两个 ($n$个) 点来确定. 阈值 $\delta$, $\gamma$.
        \Ensure 最优的直线 (平面) 参数
        \State 随机选择 $k$ 组能确定这个直线的点,也就是在所有点里面选出一个 $k\times 2$ 的矩阵.
        \State 对每一组点计算出一条直线
        \State 对每一组点的直线计算出所有点到这条直线的距离,如果小于阈值 $\delta$,则认为这个点是这条直线的 inlier
        \State 找到最大的 inlier 数量的直线,如果大于阈值 $\gamma$,则认为这条直线是最优的
        \State 对这个最优的直线,用这个直线所有的 inlier 重新使用最小二乘法计算出最优的直线参数 
    \end{algorithmic}
\end{algo}
{\kaishu 注: 可能会出现多组直线的 inliner 数量相同的情况, 此时使用所有inlier进行最小二乘法拟合是必要的.}

\subsubsection{How Many Samples?}

假设我们有所有 inliner 占比为 $w$ 的先验知识,同时希望有不低于 $p$ 的概率能够找到一个最优的直线,那么我们需要多少次迭代呢?

\begin{equation}
\mathbf{\Pr}\text{[$n$个点组成的 sample 全部是inliner]} = w^n
\end{equation}

如果一组点中有一个点是 outliner,那么我们称这组点 fail.

\begin{equation}
\mathbf{\Pr}\text{[k组点全部fail]} = {(1-w^n)}^k
\end{equation}

我们希望 k 组点全部 fail 的概率小于 $1-p$.

\begin{equation}
{(1-w^{n})}^k < 1-p
\Rightarrow
k > \frac{\log(1-p)}{\log(1-w^n)}
\end{equation}

\subsubsection{Cons and Pros}
TODO Here.

\subsection{Hough Transform}
其实就是把一条直线从实际空间的表示转换到参数空间的表示.但是如果存在垂直的直线,可能需要考虑使用极坐标来作为参数空间.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/hough1.png}
    \caption{Hough Transform w/o Noise}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/hough2.png}
    \caption{Hough Transform w/ Noise and Outliers}
\end{figure}

题外话: 传统计算机视觉基于 module-based methods, 应用了多种技巧提升鲁棒性, 但现实生活中有很多复杂的特殊情况, 
因而需要大量的 rules or post-processing. 而深度学习则是 end-to-end 的方法, 但实际应用中是二者的取舍与均衡.