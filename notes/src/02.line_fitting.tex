\section{Line Fitting}
\subsection{Least Square Method}
\begin{itemize}
    \item Data points: $(x_i, y_i)$, $i=1,2,\ldots,n$
    \item Line equation: $y_i - mx_i - b = 0$
    \item Objective: minimize the sum of squared errors ($\mathcal{L}_2$ norm): 
    \[
    \text{Energy} = \sum_{i=1}^{n} (y_i - mx_i - b)^2
    \]
\end{itemize}
对于不同的范数选择, energy landscape 会有所不同, 使用 $\mathcal{L}_2$ norm 我们可以得到一个解析解:
\begin{align*}
    E = \norm{ \mathbf{Y} - \mathbf{X} \mathbf{B} }^2 &= T^T Y - 2 (XB)^T Y + (XB)^T (XB) \\
    \frac{\partial E}{\partial B} &= - 2 X^T Y + 2 X^T X B = 0  \\
    \implies B^* &= (X^T X)^{-1} X^T Y \quad \text{where } B := [m, b]^T
\end{align*}
注: 关于矩阵求导可以参考 \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{Matrix Cookbook} 或 \href{https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf}{Matrix Calculus}.
\subsubsection{SVD Decomposition}
The original method fails completely for vertical lines. $\implies$ Use $ax + by + d = 0$ instead, then the optimization problem becomes: \marginpar{\kaishu 限制$\norm{h}=1$ 的约束为了避免平凡解.}
\begin{align*}
    \text{minimize} &\quad \norm{A h} = 0 \\ 
    \text{subject to } &\quad \norm{h} = 1
\end{align*}
where 
\[
    \renewcommand{\arraystretch}{0.85}
    A = \begin{bmatrix}
        x_1 & y_1 & 1 \\
        x_2 & y_2 & 1\\
        & \cdots &  \\
        x_n & y_n & 1
    \end{bmatrix}, \quad h = \begin{bmatrix}
        a \\ b \\ d
    \end{bmatrix}
\]
TODO Here. Add details about SVD decomposition. 

\subsubsection{Robustness}
Least square method is robust to small noises but sensitive to outliers.

$\mathcal{L}_1$ norm 对于outlier的鲁棒性会更强(梯度与残差的大小不相关), 但优化问题更困难.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/not_roboust_outliner.png}
    \caption{Least Square Method is not robust to outliers}
\end{figure}


\subsection{RANSAC}

RANSAC:RANdom SAmple Consensus
\subsubsection{Ideas and Algorithm}
\textbf{Idea:} we need to ﬁnd a line that has the largest supporters (or inliers)
\marginpar{\kaishu 在作业中会实现并行算法.}
\begin{algo}
    \centering 
    \caption{\textbf{RANSAC Loop(Sequential Version)}}
    \begin{algorithmic}[1]
        \Require 假设这个直线 (平面) 需要两个 ($n$个) 点来确定. 阈值 $\delta$, $\gamma$.
        \Ensure 最优的直线 (平面) 参数
        \State 随机选择 $k$ 组能确定这个直线的点,也就是在所有点里面选出一个 $k\times 2$ 的矩阵.
        \State 对每一组点计算出一条直线
        \State 对每一组点的直线计算出所有点到这条直线的距离,如果小于阈值 $\delta$,则认为这个点是这条直线的 inlier
        \State 找到最大的 inlier 数量的直线,如果大于阈值 $\gamma$,则认为这条直线是最优的
        \State 对这个最优的直线,用这个直线所有的 inlier 重新使用最小二乘法计算出最优的直线参数 
    \end{algorithmic}
\end{algo}
{\kaishu 注: 可能会出现多组直线的 inliner 数量相同的情况, 此时使用所有inlier进行最小二乘法拟合是必要的.}

\subsubsection{How Many Samples?}

假设我们有所有 inliner 占比为 $w$ 的先验知识,同时希望有不低于 $p$ 的概率能够找到一个最优的直线,那么我们需要多少次迭代呢?

\begin{equation}
\mathbf{\Pr}\text{[$n$个点组成的 sample 全部是inliner]} = w^n
\end{equation}

如果一组点中有一个点是 outliner,那么我们称这组点 fail.

\begin{equation}
\mathbf{\Pr}\text{[k组点全部fail]} = {(1-w^n)}^k
\end{equation}

我们希望 k 组点全部 fail 的概率小于 $1-p$.

\begin{equation}
{(1-w^{n})}^k < 1-p
\Rightarrow
k > \frac{\log(1-p)}{\log(1-w^n)}
\end{equation}

\subsubsection{Cons and Pros}
TODO Here.

\subsection{Hough Transform}
其实就是把一条直线从实际空间的表示转换到参数空间的表示.但是如果存在垂直的直线,可能需要考虑使用极坐标来作为参数空间.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/hough1.png}
    \caption{Hough Transform w/o Noise}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/hough2.png}
    \caption{Hough Transform w/ Noise and Outliers}
\end{figure}

题外话: 传统计算机视觉基于 module-based methods, 应用了多种技巧提升鲁棒性, 但现实生活中有很多复杂的特殊情况, 
因而需要大量的 rules or post-processing. 而深度学习则是 end-to-end 的方法, 但实际应用中是二者的取舍与均衡.